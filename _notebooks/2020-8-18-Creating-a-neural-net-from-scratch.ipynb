{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-8-18-Creating-a-neural-net-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrW34NgVLRTqseahn3hjbx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDiLETFB_fiE",
        "colab_type": "text"
      },
      "source": [
        "# \"Creating a neural net from scratch in Python\"\n",
        "> \"MOOC Part 2: Recreating fastai from the ground up\"\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [Misc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGjGCo8-_vGO",
        "colab_type": "text"
      },
      "source": [
        "So I'm one lesson into [part 2](https://course.fast.ai/videos/?lesson=8) of the [Fastai course (2019)](https://course.fast.ai/) on account of Fastai 2 not being quite ready for release yet.\n",
        "\n",
        "Not what I was expecting! I *was* expecting more advanced topics like object detection, style transfer, etc etc. In previous years this is what part 2 was mainly about.\n",
        "\n",
        "The 2019 version of part 2 however, starts with recreating the basics of Fastai, from scratch, in Python.\n",
        "\n",
        "You get to start with only the most basic libraries, and you don't get to use any pre-existing DL stuff until you've written it yourself from scratch, so for example you're not allowed to use `m1@m2` until you've implemented your own matmul. The first lesson starts with matmul using 3 nested loops and works it's way up using element-wise maths, broadcasting, einsum etc until you've got something (literally) about fifty thousand times faster than 3 nested loops; Then you're allowed to use `@`.\n",
        "\n",
        "It's very interesting. While it's not really necessary for you to go this deep into how neural nets work, some people just understand things better this way, and I'm one of them. Also the flip-side of 'you don't really need to know this' is that when papers and new techniques come along, if you *do* understand this then it's going to be much easier to experiment with new archtecture ideas, new activation functions, optimisations etc.\n",
        "\n",
        "So that's me for the next little while. Next Tuesday I need to pack my family back into the car and drive back across Europe to the UK, and go back to my day job. I'm not as close as I'd like to applying for DS jobs, but there's been a **lot** more to learn than I anticipated.\n",
        "\n",
        "> Note: If you're in the London or Surrey area and want to talk to me about a job, please get in touch. <joedockrill@gmail.com>"
      ]
    }
  ]
}