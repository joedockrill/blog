{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-09-13-neural-network-demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1uCq8I5z307cLObY9/8Fu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVCwFUY76Hzh",
        "colab_type": "text"
      },
      "source": [
        "# \"Neural network demo\"\n",
        "> \"A neural network in 11 lines of code\"\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [Misc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cmzO4AW6aOv",
        "colab_type": "text"
      },
      "source": [
        "Andrew Trask has an old article called [A neural network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/) which I thought was rather cute, so I knocked up a little runnable demo of the 2 layer part of the article which allows you to step through epoch by epoch and watch it calculate preds->error->delta->new weights.\n",
        "\n",
        "It’s an old article using a sigmoid activation but if you play with the demo while reading the article, that actually makes it easier to visualise what’s happening and how it updates more/less when it’s more/less certain based on the predictions’ positions on the curve.\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x,deriv=False):\n",
        "  return x*(1-x) if deriv else 1/(1+np.exp(-x))\n",
        "\n",
        "X = np.array([ [0,0,1],\n",
        "               [0,1,1],\n",
        "               [1,0,1],\n",
        "               [1,1,1] ])\n",
        "\n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "w1 = 2*np.random.random((3,1))-1         # init weights with mean 0\n",
        "\n",
        "for i in range(10000):\n",
        "  l1 = sigmoid(np.dot(X,w1))             # forward\n",
        "  l1_error = y - l1                      # error\n",
        "  l1_delta = l1_error * sigmoid(l1,True) # how much we missed * slope of the sigmoid\n",
        "  w1 += np.dot(X.T,l1_delta)             # update weights\n",
        "```\n",
        "\n",
        "You can play with a step-by-step runnable version here:\n",
        "https://joedockrill.herokuapp.com/voila/render/NeuralNetDemo.ipynb\n",
        "\n",
        "If you teach, this might be a useful visual aid for you. It's a little like the NN equivalent of [matrixmultiplication.xyz](http://matrixmultiplication.xyz/)."
      ]
    }
  ]
}